{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Installing required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Using GPU for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re\n",
    "from tqdm import tqdm_notebook\n",
    "from uuid import uuid4\n",
    "\n",
    "## Torch Modules\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 44,898\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28225</th>\n",
       "      <td>[VIDEO] YEPâ€¦GUN-CONTROL BILL SAID THAT TODAY: ...</td>\n",
       "      <td>You can t have people walking around with gun...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jun 14, 2015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31872</th>\n",
       "      <td>EU warns U.S. it may respond swiftly to counte...</td>\n",
       "      <td>BRUSSELS (Reuters) - The European Union warned...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>July 26, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31947</th>\n",
       "      <td>MARINE CORPS GENERAL WARNS U.S. TROOPS: â€œThere...</td>\n",
       "      <td>While the mainstream media remains transfixed ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Dec 23, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>Tax overhaul drama moves to Senate as House ap...</td>\n",
       "      <td>WASHINGTON (Reuters) - Congressional Republica...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 16, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14413</th>\n",
       "      <td>SUNDAY SCREENING: The Deep State: Hiding in Pl...</td>\n",
       "      <td>Our weekly documentary screening curated by th...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>September 3, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11434</th>\n",
       "      <td>THE TEA PARTY IS MAKING A COMEBACK! Conservati...</td>\n",
       "      <td>As liberal protesters step up confrontations w...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Feb 16, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>Thousands of Indonesians join anti-Myanmar ral...</td>\n",
       "      <td>JAKARTA (Reuters) - Thousands of Indonesians, ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>September 6, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36863</th>\n",
       "      <td>McCain Has The Best Laugh Heâ€™s Had In 80 Year...</td>\n",
       "      <td>It s gotta be hard these days to be John McCai...</td>\n",
       "      <td>News</td>\n",
       "      <td>October 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18491</th>\n",
       "      <td>POLL: Most Trump Supporters Say Fake â€˜Bowling...</td>\n",
       "      <td>Did you know that people who support Donald Tr...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 10, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24290</th>\n",
       "      <td>Supreme Court faces 4-4 split in Obamacare con...</td>\n",
       "      <td>WASHINGTON (Reuters) - The Supreme Court on We...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>March 23, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "28225  [VIDEO] YEPâ€¦GUN-CONTROL BILL SAID THAT TODAY: ...   \n",
       "31872  EU warns U.S. it may respond swiftly to counte...   \n",
       "31947  MARINE CORPS GENERAL WARNS U.S. TROOPS: â€œThere...   \n",
       "6712   Tax overhaul drama moves to Senate as House ap...   \n",
       "14413  SUNDAY SCREENING: The Deep State: Hiding in Pl...   \n",
       "11434  THE TEA PARTY IS MAKING A COMEBACK! Conservati...   \n",
       "4041   Thousands of Indonesians join anti-Myanmar ral...   \n",
       "36863   McCain Has The Best Laugh Heâ€™s Had In 80 Year...   \n",
       "18491   POLL: Most Trump Supporters Say Fake â€˜Bowling...   \n",
       "24290  Supreme Court faces 4-4 split in Obamacare con...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "28225   You can t have people walking around with gun...     left-news   \n",
       "31872  BRUSSELS (Reuters) - The European Union warned...  politicsNews   \n",
       "31947  While the mainstream media remains transfixed ...      politics   \n",
       "6712   WASHINGTON (Reuters) - Congressional Republica...  politicsNews   \n",
       "14413  Our weekly documentary screening curated by th...   Middle-east   \n",
       "11434  As liberal protesters step up confrontations w...      politics   \n",
       "4041   JAKARTA (Reuters) - Thousands of Indonesians, ...     worldnews   \n",
       "36863  It s gotta be hard these days to be John McCai...          News   \n",
       "18491  Did you know that people who support Donald Tr...          News   \n",
       "24290  WASHINGTON (Reuters) - The Supreme Court on We...  politicsNews   \n",
       "\n",
       "                     date  target  \n",
       "28225        Jun 14, 2015       0  \n",
       "31872      July 26, 2017        1  \n",
       "31947        Dec 23, 2017       0  \n",
       "6712   November 16, 2017        1  \n",
       "14413   September 3, 2017       0  \n",
       "11434        Feb 16, 2017       0  \n",
       "4041   September 6, 2017        1  \n",
       "36863    October 30, 2017       0  \n",
       "18491   February 10, 2017       0  \n",
       "24290     March 23, 2016        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# import emoji\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_true = pd.read_csv('True.csv',encoding='UTF-8')\n",
    "df_fake = pd.read_csv('Fake.csv',encoding='UTF-8')\n",
    "\n",
    "# creating target variable\n",
    "df_true['target'] = 1\n",
    "df_fake['target'] = 0\n",
    "\n",
    "# concatenating in one single dataframe\n",
    "train_df = df_true.append(df_fake)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(train_df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cambodia deports 61 telecom extortion scam suspects'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the lists of sentences and labels\n",
    "sentences = train_df['title'].values\n",
    "labels = train_df['target'].values\n",
    "train_df['title'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base models loaded\n"
     ]
    }
   ],
   "source": [
    "# loading pre-trained models\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "#     TFBertForSequenceClassification, \n",
    "                          BertTokenizer,\n",
    "#                           TFRobertaForSequenceClassification,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer,\n",
    "                         AdamW)\n",
    "\n",
    "# BERT\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                                                                num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                                                                                # You can increase this for multi-class tasks.   \n",
    "                                                                output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                                output_hidden_states = False # Whether the model returns all hidden-states.\n",
    "                                                          )\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "bert_model.cuda()\n",
    "                                                           \n",
    "\n",
    "# RoBERTa\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", # 12-layer, 768-hidden, 12-heads, 125M parameters RoBERTa using the BERT-base architecture\n",
    "                                                                    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                                                                                    # You can increase this for multi-class tasks.   \n",
    "                                                                    output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                                    output_hidden_states = False # Whether the model returns all hidden-states.\n",
    "                                                                )\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "roberta_model.cuda()\n",
    "\n",
    "print(' Base models loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:   BUSTED: Trump Running Mate Mike Pence Caught Suppressing Up To 10,000 Black Votes\n",
      "Tokenized BERT:  ['busted', ':', 'trump', 'running', 'mate', 'mike', 'pen', '##ce', 'caught', 'suppress', '##ing', 'up', 'to', '10', ',', '000', 'black', 'votes']\n",
      "Token IDs BERT:  [23142, 1024, 8398, 2770, 6775, 3505, 7279, 3401, 3236, 16081, 2075, 2039, 2000, 2184, 1010, 2199, 2304, 4494]\n",
      "Tokenized RoBERT:  ['Ä B', 'UST', 'ED', ':', 'Ä Trump', 'Ä Running', 'Ä Mate', 'Ä Mike', 'Ä Pence', 'Ä C', 'aught', 'Ä Supp', 'ressing', 'Ä Up', 'Ä To', 'Ä 10', ',', '000', 'Ä Black', 'Ä Votes']\n",
      "Token IDs RoBERTa:  [163, 11120, 1691, 35, 140, 15552, 12869, 1483, 6913, 230, 19906, 13256, 9828, 3105, 598, 158, 6, 151, 1378, 39347]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "print('Tokenized BERT: ', bert_tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the tweet mapped to token ids.\n",
    "print('Token IDs BERT: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "print('Tokenized RoBERT: ', roberta_tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the tweet mapped to token ids.\n",
    "print('Token IDs RoBERTa: ', roberta_tokenizer.convert_tokens_to_ids(roberta_tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will Your Hometown Be Taking In Obamaâ€™s Refugees? Hereâ€™s The List Of Cities Where Theyâ€™re Being Transplanted ðŸ™ï¸\n",
      "BERT: ['will', 'your', 'hometown', 'be', 'taking', 'in', 'obama', 'â€™', 's', 'refugees', '?', 'here', 'â€™', 's', 'the', 'list', 'of', 'cities', 'where', 'they', 'â€™', 're', 'being', 'transplant', '##ed', '[UNK]']\n",
      "RoBERTa: ['Will', 'Ä Your', 'Ä H', 'ometown', 'Ä Be', 'Ä Taking', 'Ä In', 'Ä Obama', 'Ã¢Ä¢', 'Ä»', 's', 'Ä Refugees', '?', 'Ä Here', 'Ã¢Ä¢', 'Ä»', 's', 'Ä The', 'Ä List', 'Ä Of', 'Ä Cities', 'Ä Where', 'Ä They', 'Ã¢Ä¢', 'Ä»', 're', 'Ä Being', 'Ä Trans', 'pl', 'anted', 'Ä Ã°Å', 'Ä±', 'Ä»', 'Ã¯Â¸Ä±']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"\"\"Will Your Hometown Be Taking In Obamaâ€™s Refugees? Hereâ€™s The List Of Cities Where Theyâ€™re Being Transplanted ðŸ™ï¸\"\"\"\n",
    "\n",
    "bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)\n",
    "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
    "\n",
    "print(\"\"\"Will Your Hometown Be Taking In Obamaâ€™s Refugees? Hereâ€™s The List Of Cities Where Theyâ€™re Being Transplanted ðŸ™ï¸\"\"\")\n",
    "print(\"BERT:\", bert_tokenized_sequence)\n",
    "print(\"RoBERTa:\", roberta_tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length BERT:  63\n",
      "Max sentence length RoBERTa:  83\n"
     ]
    }
   ],
   "source": [
    "max_len_bert = 0\n",
    "max_len_roberta = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids_bert = bert_tokenizer.encode(sent, add_special_tokens=True)\n",
    "    input_ids_roberta = roberta_tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len_bert = max(max_len_bert, len(input_ids_bert))\n",
    "    max_len_roberta = max(max_len_roberta, len(input_ids_roberta))\n",
    "\n",
    "    \n",
    "print('Max sentence length BERT: ', max_len_bert)\n",
    "print('Max sentence length RoBERTa: ', max_len_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Tommy\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  WOW! BARBARA BUSH Will Be Keynote Speaker For Baby-Killing Planned Parenthood Fundraiser\n",
      "Token IDs BERT: tensor([  101, 10166,   999,  6437,  5747,  2097,  2022, 25569,  5882,  2005,\n",
      "         3336,  1011,  4288,  3740,  6687,  9021, 28536,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Token IDs RoBERTa: tensor([    0,   771,  4581,   328, 26676,   387, 19142,   163, 29397,  2290,\n",
      "         1456,  3350,  3892,  6457,  6358,   286, 10517,    12,   530,  7491,\n",
      "        15121, 15805,  2896,   763,  5999,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "bert_input_ids = []\n",
    "bert_attention_masks = []\n",
    "roberta_input_ids = []\n",
    "roberta_attention_masks = []\n",
    "sentence_ids = []\n",
    "counter = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    bert_encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 120,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    \n",
    "    roberta_encoded_dict = roberta_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 120,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    # Add the encoded sentence to the list.    \n",
    "    bert_input_ids.append(bert_encoded_dict['input_ids'])\n",
    "    roberta_input_ids.append(roberta_encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    bert_attention_masks.append(bert_encoded_dict['attention_mask'])\n",
    "    roberta_attention_masks.append(roberta_encoded_dict['attention_mask'])\n",
    "    \n",
    "    # collecting sentence_ids\n",
    "    sentence_ids.append(counter)\n",
    "    counter  = counter + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "bert_input_ids = torch.cat(bert_input_ids, dim=0)\n",
    "bert_attention_masks = torch.cat(bert_attention_masks, dim=0)\n",
    "\n",
    "roberta_input_ids = torch.cat(roberta_input_ids, dim=0)\n",
    "roberta_attention_masks = torch.cat(roberta_attention_masks, dim=0)\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "sentence_ids = torch.tensor(sentence_ids)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[1])\n",
    "print('Token IDs BERT:', bert_input_ids[1])\n",
    "print('Token IDs RoBERTa:', roberta_input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "# function to seed the script globally\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "bert_dataset = TensorDataset(sentence_ids, bert_input_ids, bert_attention_masks, labels)\n",
    "roberta_dataset = TensorDataset(roberta_input_ids, roberta_attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  101, 23142,  1024,  8398,  2770,  6775,  3505,  7279,  3401,  3236,\n",
       "         16081,  2075,  2039,  2000,  2184,  1010,  2199,  2304,  4494,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to remove sentice ids from the tensor dataset post train test split\n",
    "def index_remover(tensordata):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "   \n",
    "    for a,b,c,d in tensordata:\n",
    "        input_ids.append(b.tolist())\n",
    "        attention_masks.append(c.tolist())\n",
    "        labels.append(d.tolist())\n",
    "        \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    final_dataset =  TensorDataset(input_ids, attention_masks, labels)\n",
    "    return final_dataset\n",
    "        \n",
    "# check\n",
    "trial_dataset =  index_remover(bert_dataset)\n",
    "trial_dataset[0]\n",
    "# yes we were able to remove the sentence id from the data without disturbing the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40,408 training samples\n",
      "19,230 training samples with real disater tweets\n",
      "4,490 validation samples\n",
      "2,187 validation samples with real disater tweets\n"
     ]
    }
   ],
   "source": [
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(bert_dataset))\n",
    "val_size = len(bert_dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "bert_train_dataset, bert_val_dataset = random_split(bert_dataset, [train_size, val_size])\n",
    "roberta_train_dataset, roberta_val_dataset = random_split(roberta_dataset, [train_size, val_size])\n",
    "\n",
    "# Checking whether the distribution of target is consitent across both the sets\n",
    "sentence_ids_list_valid = []\n",
    "for a,b,c,d in bert_val_dataset:\n",
    "  sentence_ids_list_valid.append(a.tolist())\n",
    "\n",
    "# removing sentence ids from tensor dataset so that it can be used for training \n",
    "bert_train_dataset = index_remover(bert_train_dataset)\n",
    "bert_val_dataset = index_remover(bert_val_dataset)\n",
    "\n",
    "# Checking whether the distribution of target is consitent across both the sets\n",
    "label_temp_list = []\n",
    "for a,b,c in bert_train_dataset:\n",
    "  label_temp_list.append(c)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} training samples with real disater tweets'.format(sum(label_temp_list)))\n",
    "\n",
    "\n",
    "label_temp_list = []\n",
    "for a,b,c in bert_val_dataset:\n",
    "  label_temp_list.append(c)\n",
    "\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} validation samples with real disater tweets'.format(sum(label_temp_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "bert_train_dataloader = DataLoader(\n",
    "            bert_train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(bert_train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "roberta_train_dataloader = DataLoader(\n",
    "            roberta_train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(roberta_train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "bert_validation_dataloader = DataLoader(\n",
    "            bert_val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(bert_val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "roberta_validation_dataloader = DataLoader(\n",
    "            roberta_val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(roberta_val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RoBERTa model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
      "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
      "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
      "roberta.embeddings.LayerNorm.weight                           (768,)\n",
      "roberta.embeddings.LayerNorm.bias                             (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
      "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
      "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
      "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
      "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
      "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
      "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
      "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.dense.weight                                   (768, 768)\n",
      "classifier.dense.bias                                         (768,)\n",
      "classifier.out_proj.weight                                  (2, 768)\n",
      "classifier.out_proj.bias                                        (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the roberta_model's parameters as a list of tuples.\n",
    "params = list(roberta_model.named_parameters())\n",
    "\n",
    "print('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "bert_optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "roberta_optimizer = AdamW(roberta_model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 2,I have already seen that the model starts overfitting beyound 2 epochs\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(bert_train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "roberta_scheduler = get_linear_schedule_with_warmup(roberta_optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,263.    Elapsed: 0:00:09.\n",
      "  Batch    80  of  1,263.    Elapsed: 0:00:17.\n",
      "  Batch   120  of  1,263.    Elapsed: 0:00:26.\n",
      "  Batch   160  of  1,263.    Elapsed: 0:00:35.\n",
      "  Batch   200  of  1,263.    Elapsed: 0:00:43.\n",
      "  Batch   240  of  1,263.    Elapsed: 0:00:51.\n",
      "  Batch   280  of  1,263.    Elapsed: 0:00:59.\n",
      "  Batch   320  of  1,263.    Elapsed: 0:01:07.\n",
      "  Batch   360  of  1,263.    Elapsed: 0:01:15.\n",
      "  Batch   400  of  1,263.    Elapsed: 0:01:23.\n",
      "  Batch   440  of  1,263.    Elapsed: 0:01:31.\n",
      "  Batch   480  of  1,263.    Elapsed: 0:01:39.\n",
      "  Batch   520  of  1,263.    Elapsed: 0:01:47.\n",
      "  Batch   560  of  1,263.    Elapsed: 0:01:55.\n",
      "  Batch   600  of  1,263.    Elapsed: 0:02:03.\n",
      "  Batch   640  of  1,263.    Elapsed: 0:02:11.\n",
      "  Batch   680  of  1,263.    Elapsed: 0:02:19.\n",
      "  Batch   720  of  1,263.    Elapsed: 0:02:27.\n",
      "  Batch   760  of  1,263.    Elapsed: 0:02:35.\n",
      "  Batch   800  of  1,263.    Elapsed: 0:02:43.\n",
      "  Batch   840  of  1,263.    Elapsed: 0:02:51.\n",
      "  Batch   880  of  1,263.    Elapsed: 0:02:59.\n",
      "  Batch   920  of  1,263.    Elapsed: 0:03:07.\n",
      "  Batch   960  of  1,263.    Elapsed: 0:03:15.\n",
      "  Batch 1,000  of  1,263.    Elapsed: 0:03:22.\n",
      "  Batch 1,040  of  1,263.    Elapsed: 0:03:30.\n",
      "  Batch 1,080  of  1,263.    Elapsed: 0:03:38.\n",
      "  Batch 1,120  of  1,263.    Elapsed: 0:03:46.\n",
      "  Batch 1,160  of  1,263.    Elapsed: 0:03:54.\n",
      "  Batch 1,200  of  1,263.    Elapsed: 0:04:02.\n",
      "  Batch 1,240  of  1,263.    Elapsed: 0:04:10.\n",
      "\n",
      "  Average training loss: 0.07\n",
      "  Training epcoh took: 0:04:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,263.    Elapsed: 0:00:08.\n",
      "  Batch    80  of  1,263.    Elapsed: 0:00:16.\n",
      "  Batch   120  of  1,263.    Elapsed: 0:00:24.\n",
      "  Batch   160  of  1,263.    Elapsed: 0:00:32.\n",
      "  Batch   200  of  1,263.    Elapsed: 0:00:40.\n",
      "  Batch   240  of  1,263.    Elapsed: 0:00:48.\n",
      "  Batch   280  of  1,263.    Elapsed: 0:00:56.\n",
      "  Batch   320  of  1,263.    Elapsed: 0:01:04.\n",
      "  Batch   360  of  1,263.    Elapsed: 0:01:11.\n",
      "  Batch   400  of  1,263.    Elapsed: 0:01:19.\n",
      "  Batch   440  of  1,263.    Elapsed: 0:01:27.\n",
      "  Batch   480  of  1,263.    Elapsed: 0:01:35.\n",
      "  Batch   520  of  1,263.    Elapsed: 0:01:43.\n",
      "  Batch   560  of  1,263.    Elapsed: 0:01:51.\n",
      "  Batch   600  of  1,263.    Elapsed: 0:01:59.\n",
      "  Batch   640  of  1,263.    Elapsed: 0:02:07.\n",
      "  Batch   680  of  1,263.    Elapsed: 0:02:15.\n",
      "  Batch   720  of  1,263.    Elapsed: 0:02:23.\n",
      "  Batch   760  of  1,263.    Elapsed: 0:02:31.\n",
      "  Batch   800  of  1,263.    Elapsed: 0:02:40.\n",
      "  Batch   840  of  1,263.    Elapsed: 0:02:48.\n",
      "  Batch   880  of  1,263.    Elapsed: 0:02:56.\n",
      "  Batch   920  of  1,263.    Elapsed: 0:03:05.\n",
      "  Batch   960  of  1,263.    Elapsed: 0:03:13.\n",
      "  Batch 1,000  of  1,263.    Elapsed: 0:03:22.\n",
      "  Batch 1,040  of  1,263.    Elapsed: 0:03:31.\n",
      "  Batch 1,080  of  1,263.    Elapsed: 0:03:41.\n",
      "  Batch 1,120  of  1,263.    Elapsed: 0:03:52.\n",
      "  Batch 1,160  of  1,263.    Elapsed: 0:04:02.\n",
      "  Batch 1,200  of  1,263.    Elapsed: 0:04:12.\n",
      "  Batch 1,240  of  1,263.    Elapsed: 0:04:23.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:04:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.05\n",
      "  Validation took: 0:00:12\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:04 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "bert_training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the bert_model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-bert_model-train-do-in-pytorch)\n",
    "    bert_model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(bert_train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        bert_model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the bert_model on this training batch).\n",
    "        # The documentation for this `bert_model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/bert_model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # are given and what flags are set. For our usage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the bert_model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = bert_model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The bert_optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        bert_optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        bert_scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the bert_model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in bert_validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # Get the \"logits\" output by the bert_model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = bert_model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(bert_validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(bert_validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    bert_training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,263.    Elapsed: 0:00:10.\n",
      "  Batch    80  of  1,263.    Elapsed: 0:00:21.\n",
      "  Batch   120  of  1,263.    Elapsed: 0:00:31.\n",
      "  Batch   160  of  1,263.    Elapsed: 0:00:41.\n",
      "  Batch   200  of  1,263.    Elapsed: 0:00:52.\n",
      "  Batch   240  of  1,263.    Elapsed: 0:01:02.\n",
      "  Batch   280  of  1,263.    Elapsed: 0:01:11.\n",
      "  Batch   320  of  1,263.    Elapsed: 0:01:22.\n",
      "  Batch   360  of  1,263.    Elapsed: 0:01:31.\n",
      "  Batch   400  of  1,263.    Elapsed: 0:01:40.\n",
      "  Batch   440  of  1,263.    Elapsed: 0:01:48.\n",
      "  Batch   480  of  1,263.    Elapsed: 0:01:57.\n",
      "  Batch   520  of  1,263.    Elapsed: 0:02:06.\n",
      "  Batch   560  of  1,263.    Elapsed: 0:02:16.\n",
      "  Batch   600  of  1,263.    Elapsed: 0:02:26.\n",
      "  Batch   640  of  1,263.    Elapsed: 0:02:37.\n",
      "  Batch   680  of  1,263.    Elapsed: 0:02:47.\n",
      "  Batch   720  of  1,263.    Elapsed: 0:02:57.\n",
      "  Batch   760  of  1,263.    Elapsed: 0:03:07.\n",
      "  Batch   800  of  1,263.    Elapsed: 0:03:16.\n",
      "  Batch   840  of  1,263.    Elapsed: 0:03:25.\n",
      "  Batch   880  of  1,263.    Elapsed: 0:03:33.\n",
      "  Batch   920  of  1,263.    Elapsed: 0:03:42.\n",
      "  Batch   960  of  1,263.    Elapsed: 0:03:51.\n",
      "  Batch 1,000  of  1,263.    Elapsed: 0:04:00.\n",
      "  Batch 1,040  of  1,263.    Elapsed: 0:04:09.\n",
      "  Batch 1,080  of  1,263.    Elapsed: 0:04:20.\n",
      "  Batch 1,120  of  1,263.    Elapsed: 0:04:30.\n",
      "  Batch 1,160  of  1,263.    Elapsed: 0:04:40.\n",
      "  Batch 1,200  of  1,263.    Elapsed: 0:04:50.\n",
      "  Batch 1,240  of  1,263.    Elapsed: 0:05:01.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:05:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:00:10\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of  1,263.    Elapsed: 0:00:08.\n",
      "  Batch    80  of  1,263.    Elapsed: 0:00:16.\n",
      "  Batch   120  of  1,263.    Elapsed: 0:00:24.\n",
      "  Batch   160  of  1,263.    Elapsed: 0:00:33.\n",
      "  Batch   200  of  1,263.    Elapsed: 0:00:42.\n",
      "  Batch   240  of  1,263.    Elapsed: 0:00:51.\n",
      "  Batch   280  of  1,263.    Elapsed: 0:00:59.\n",
      "  Batch   320  of  1,263.    Elapsed: 0:01:07.\n",
      "  Batch   360  of  1,263.    Elapsed: 0:01:16.\n",
      "  Batch   400  of  1,263.    Elapsed: 0:01:25.\n",
      "  Batch   440  of  1,263.    Elapsed: 0:01:34.\n",
      "  Batch   480  of  1,263.    Elapsed: 0:01:42.\n",
      "  Batch   520  of  1,263.    Elapsed: 0:01:50.\n",
      "  Batch   560  of  1,263.    Elapsed: 0:01:59.\n",
      "  Batch   600  of  1,263.    Elapsed: 0:02:07.\n",
      "  Batch   640  of  1,263.    Elapsed: 0:02:15.\n",
      "  Batch   680  of  1,263.    Elapsed: 0:02:23.\n",
      "  Batch   720  of  1,263.    Elapsed: 0:02:31.\n",
      "  Batch   760  of  1,263.    Elapsed: 0:02:39.\n",
      "  Batch   800  of  1,263.    Elapsed: 0:02:47.\n",
      "  Batch   840  of  1,263.    Elapsed: 0:02:56.\n",
      "  Batch   880  of  1,263.    Elapsed: 0:03:04.\n",
      "  Batch   920  of  1,263.    Elapsed: 0:03:12.\n",
      "  Batch   960  of  1,263.    Elapsed: 0:03:20.\n",
      "  Batch 1,000  of  1,263.    Elapsed: 0:03:28.\n",
      "  Batch 1,040  of  1,263.    Elapsed: 0:03:36.\n",
      "  Batch 1,080  of  1,263.    Elapsed: 0:03:44.\n",
      "  Batch 1,120  of  1,263.    Elapsed: 0:03:52.\n",
      "  Batch 1,160  of  1,263.    Elapsed: 0:04:01.\n",
      "  Batch 1,200  of  1,263.    Elapsed: 0:04:09.\n",
      "  Batch 1,240  of  1,263.    Elapsed: 0:04:17.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:04:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 1.00\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:46 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "roberta_training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the roberta_model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-roberta_model-train-do-in-pytorch)\n",
    "    roberta_model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(roberta_train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(roberta_train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        roberta_model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the roberta_model on this training batch).\n",
    "        # The documentation for this `roberta_model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/roberta_model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments\n",
    "        # are given and what flags are set. For our usage here, it returns\n",
    "        # the loss (because we provided labels) and the \"logits\"--the roberta_model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = roberta_model(b_input_ids, \n",
    "#                              token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(roberta_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The roberta_optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        roberta_optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        roberta_scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(roberta_train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the roberta_model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    roberta_model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in roberta_validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # Get the \"logits\" output by the roberta_model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = roberta_model(b_input_ids, \n",
    "#                                    token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(roberta_validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(roberta_validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    roberta_training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.40e-02</td>\n",
       "      <td>7.32e-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0:05:06</td>\n",
       "      <td>0:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.04e-03</td>\n",
       "      <td>2.63e-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0:04:22</td>\n",
       "      <td>0:00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1           1.40e-02     7.32e-03            1.0       0:05:06         0:00:10\n",
       "2           3.04e-03     2.63e-03            1.0       0:04:22         0:00:09"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=roberta_training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Detalied Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df  = train_df.iloc[sentence_ids_list_valid,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 4,490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.title.values\n",
    "labels = df.target.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = roberta_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 75,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #padding = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "# prediction_data = TensorDataset(input_ids, attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 4,490 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "roberta_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "#   b_input_ids, b_input_mask = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = roberta_model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 2187 of 4490 (48.71%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.target.sum(), len(df.target), (df.target.sum() / len(df.target) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-faa1cab02f22>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['target'] = flat_predictions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mcc': 1.0, 'tp': 2187, 'tn': 2303, 'fp': 0, 'fn': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# adding to the main datframe\n",
    "df['target'] = flat_predictions\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "# mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "get_eval_report(flat_true_labels, flat_predictions)\n",
    "# print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "\n",
    "# from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "\n",
    "# from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "#                               TensorDataset)\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# from tools import *\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# import convert_examples_to_features\n",
    "\n",
    "# from tqdm import tqdm_notebook, trange\n",
    "# import os\n",
    "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "# from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "# # OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "BERT_MODEL = 'yelp.tar.gz'\n",
    "\n",
    "# The name of the task to train.I'm going to name this 'yelp'.\n",
    "TASK_NAME = 'yelp'\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_reports/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "CACHE_DIR = 'cache/'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_report(task_name, labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }\n",
    "\n",
    "def compute_metrics(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(task_name, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-cdc882c20574>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(bert_validation_dataloader, desc=\"Evaluating\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69ec1e23b024a1eb37b9e33ac0813cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=141.0, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-cdc882c20574>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_validation_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluating\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0minput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "bert_model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(bert_validation_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "    # create eval loss and other metric required by the task\n",
    "    if OUTPUT_MODE == \"classification\":\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "    elif OUTPUT_MODE == \"regression\":\n",
    "        loss_fct = MSELoss()\n",
    "        tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if len(preds) == 0:\n",
    "        preds.append(logits.detach().cpu().numpy())\n",
    "    else:\n",
    "        preds[0] = np.append(\n",
    "            preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds = preds[0]\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    preds = np.squeeze(preds)\n",
    "result = compute_metrics(TASK_NAME, all_label_ids.numpy(), preds)\n",
    "\n",
    "result['eval_loss'] = eval_loss\n",
    "\n",
    "output_eval_file = os.path.join(REPORTS_DIR, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
